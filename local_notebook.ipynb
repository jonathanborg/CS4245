{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add github source as a path to be accessed by kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T09:57:57.277844Z",
     "iopub.status.busy": "2022-05-12T09:57:57.277234Z",
     "iopub.status.idle": "2022-05-12T09:57:57.282584Z",
     "shell.execute_reply": "2022-05-12T09:57:57.281778Z",
     "shell.execute_reply.started": "2022-05-12T09:57:57.277809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Specifying which model to train & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_to_train = 'wgan-gp'\n",
    "# model_to_train = 'wgan-gp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Configurations specific to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    # environment\n",
    "    'environment': 'local',\n",
    "    'local_results_directory': './results',\n",
    "    'experiment_name': 'v1.6',\n",
    "    'data_directory': './data/faces_reduced',\n",
    "    'evaluation': True,\n",
    "    'num_workers': 0,\n",
    "\n",
    "    # network\n",
    "    'noise_size': 100,\n",
    "    'noise_type': 'normal', # uniform / normal\n",
    "    'discriminator_feature_map_depth': 64,\n",
    "    'generator_feature_map_depth': 64,\n",
    "\n",
    "    # training\n",
    "    'save_checkpoint_every': 10,\n",
    "    'save_image_every': 10,\n",
    "    'save_metrics_every': 10,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'discriminator_lr': 0.002,\n",
    "    'discriminator_betas': (0.5, 0.999),\n",
    "    'generator_lr': 0.002,\n",
    "    'generator_betas': (0.5, 0.999),\n",
    "    'true_label_value': 1,\n",
    "    'fake_label_value': 0,\n",
    "\n",
    "    # model\n",
    "    'model_name': model_to_train,\n",
    "\n",
    "    # model specific settings\n",
    "    # wgan settings\n",
    "    'weight_clip': 0.1,\n",
    "\n",
    "    # wgan-gp settings\n",
    "    'critic_iterations': 5,\n",
    "    'lambda_gp': 10,\n",
    "    'wgan_gp_lr': 1e-4,\n",
    "    'wgan_gp_betas': (0.0, 0.9)\n",
    "\n",
    "}\n",
    "\n",
    "# create paths\n",
    "if not os.path.isdir(config['local_results_directory']):\n",
    "    os.mkdir(config['local_results_directory'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Executing based on the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1.])\n",
      "tensor([-0.1889,  0.8942,  1.9139, -0.7609,  1.7101,  1.8918,  1.4691,  1.5987,\n",
      "         1.1090, -0.4258,  0.9387,  0.2920,  1.2825,  2.5208,  1.8043,  1.9969,\n",
      "         1.2420,  1.1446,  0.7551,  2.1238,  1.0311],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([-0.,  1.,  2., -1.,  2.,  2.,  1.,  2.,  1., -0.,  1.,  0.,  1.,  3.,\n",
      "         2.,  2.,  1.,  1.,  1.,  2.,  1.], grad_fn=<RoundBackward0>)\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 0.0081, -0.6171, -1.0463, -0.4368,  0.1799, -1.0544, -0.4031, -1.8607,\n",
      "        -1.6785, -0.5161,  1.1321,  0.0252, -0.8810, -0.6366, -0.7090, -0.2673,\n",
      "        -1.1872, -0.3035, -0.7169, -0.1758, -0.0563],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([ 0., -1., -1., -0.,  0., -1., -0., -2., -2., -1.,  1.,  0., -1., -1.,\n",
      "        -1., -0., -1., -0., -1., -0., -0.], grad_fn=<RoundBackward0>)\n",
      "\n",
      "tensor([-0.4062, -1.1001, -1.4906, -0.9456, -0.2749, -1.5214, -0.9720, -2.2970,\n",
      "        -2.2427, -0.9500,  0.6516, -0.4341, -1.3986, -1.1537, -1.1540, -0.7413,\n",
      "        -1.7624, -0.8484, -1.2215, -0.7147, -0.5834],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([-0., -1., -1., -1., -0., -2., -1., -2., -2., -1.,  1., -0., -1., -1.,\n",
      "        -1., -1., -2., -1., -1., -1., -1.], grad_fn=<RoundBackward0>)\n",
      "1/20: FID: 0.009295568456782732\tLoss_G: 0.0489\tReal_Loss_D: 1.0305\tFake_Loss_D: 0.0000\tAccuracy_G: 0.0476\tReal_Accuracy_D: 0.4286\tFake_Accuracy_D: 0.4286\n",
      "-> Saving model checkpoint\n",
      "-> Saving model images\n",
      "-> Saving metrics\n",
      "EPOCH:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/fransdeboer/Projects/CS4245/local_notebook.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/CS4245/local_notebook.ipynb#ch0000007?line=54'>55</a>\u001b[0m criterion \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/CS4245/local_notebook.ipynb#ch0000007?line=56'>57</a>\u001b[0m experiment \u001b[39m=\u001b[39m wgan_gp\u001b[39m.\u001b[39mTraining(generator, critic, generator_optimizer, critic_optimizer, device, dataloader, config)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/CS4245/local_notebook.ipynb#ch0000007?line=57'>58</a>\u001b[0m experiment\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Projects/CS4245/models_wgan/wgan_gp.py:208\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=205'>206</a>\u001b[0m critic_fake \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(fake)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=206'>207</a>\u001b[0m critic_real \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(real)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=207'>208</a>\u001b[0m gp \u001b[39m=\u001b[39m gradient_penalty(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic, real, fake, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=209'>210</a>\u001b[0m \u001b[39m# extra '-' because originally we want to maximize, so we minimize the negative.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=210'>211</a>\u001b[0m \u001b[39m# LAMDA_GP * gp is the addition for WGAN-GP\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=211'>212</a>\u001b[0m loss_critic \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(th\u001b[39m.\u001b[39mmean(critic_real) \u001b[39m-\u001b[39m th\u001b[39m.\u001b[39mmean(critic_fake)) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_gp \u001b[39m*\u001b[39m gp\n",
      "File \u001b[0;32m~/Projects/CS4245/models_wgan/wgan_gp.py:118\u001b[0m, in \u001b[0;36mgradient_penalty\u001b[0;34m(critic, real, fake, device)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=114'>115</a>\u001b[0m interpolated_images \u001b[39m=\u001b[39m real \u001b[39m*\u001b[39m alpha \u001b[39m+\u001b[39m fake \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=116'>117</a>\u001b[0m \u001b[39m# Critic score of interpolated image\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=117'>118</a>\u001b[0m mixed_scores \u001b[39m=\u001b[39m critic(interpolated_images)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=119'>120</a>\u001b[0m \u001b[39m# Take gradients of scores with respect to the interpolated images\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=120'>121</a>\u001b[0m gradient \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=121'>122</a>\u001b[0m     inputs\u001b[39m=\u001b[39minterpolated_images,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=122'>123</a>\u001b[0m     outputs\u001b[39m=\u001b[39mmixed_scores,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=125'>126</a>\u001b[0m     retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=126'>127</a>\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/CS4245/models_wgan/wgan_gp.py:61\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=60'>61</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x)\n\u001b[1;32m     <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=61'>62</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/CS4245/models_wgan/wgan_gp.py:45\u001b[0m, in \u001b[0;36mCriticBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> <a href='file:///Users/fransdeboer/Projects/CS4245/models_wgan/wgan_gp.py?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x)\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=160'>161</a>\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=162'>163</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=163'>164</a>\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=164'>165</a>\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=165'>166</a>\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=166'>167</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=167'>168</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=168'>169</a>\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=169'>170</a>\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=170'>171</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=171'>172</a>\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=172'>173</a>\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=173'>174</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=174'>175</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=175'>176</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=176'>177</a>\u001b[0m     bn_training,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=177'>178</a>\u001b[0m     exponential_average_factor,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=178'>179</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py?line=179'>180</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2417'>2418</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2418'>2419</a>\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2420'>2421</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2421'>2422</a>\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   <a href='file:///Users/fransdeboer/Projects/CS4245/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2422'>2423</a>\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import Generator, Discriminator\n",
    "import models_wgan.wgan_gp as wgan_gp\n",
    "from utils import weights_init\n",
    "from experiments import Experiment\n",
    "\n",
    "# create device\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create dataset\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])\n",
    "dataset = torchvision.datasets.ImageFolder(config['data_directory'], transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
    "\n",
    "if config['model_name'] == 'dcgan':\n",
    "    # create networks\n",
    "    generator = Generator(config['noise_size'],config['generator_feature_map_depth']).to(device)\n",
    "    discriminator = Discriminator(config['discriminator_feature_map_depth']).to(device)\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "\n",
    "    # create optimizers\n",
    "    discriminator_optimizer = th.optim.Adam(discriminator.parameters(), lr=config['discriminator_lr'], betas=config['discriminator_betas'])\n",
    "    generator_optimizer = th.optim.Adam(generator.parameters(), lr=config['generator_lr'], betas=config['generator_betas'])\n",
    "\n",
    "    # create loss\n",
    "    criterion = th.nn.BCELoss()\n",
    "    # create experiment\n",
    "    experiment = Experiment(config, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, dataloader)\n",
    "    print('Training dcgan')\n",
    "    experiment.train()\n",
    "\n",
    "\n",
    "# elif config['model_name'] == 'dcgan-data-aug':\n",
    "#\n",
    "# elif config['model_name'] == 'wgan':\n",
    "#\n",
    "elif config['model_name'] == 'wgan-gp':\n",
    "    # create networks\n",
    "    generator = wgan_gp.Generator(config[\"noise_size\"], config[\"generator_feature_map_depth\"]).to(device)\n",
    "    critic = wgan_gp.Critic(config[\"discriminator_feature_map_depth\"]).to(device)\n",
    "    generator.apply(weights_init)\n",
    "    critic.apply(weights_init)\n",
    "\n",
    "    # create optimizers\n",
    "    # Optimizer (WGAN uses RMSprop, WGAN-GP uses Adam)\n",
    "    critic_optimizer = th.optim.Adam(critic.parameters(), lr=config[\"wgan_gp_lr\"], betas=config[\"wgan_gp_betas\"])\n",
    "    generator_optimizer = th.optim.Adam(generator.parameters(), lr=config[\"wgan_gp_lr\"], betas=config[\"wgan_gp_betas\"])\n",
    "\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "    criterion = None\n",
    "\n",
    "    experiment = wgan_gp.Training(generator, critic, generator_optimizer, critic_optimizer, device, dataloader, config)\n",
    "    experiment.train()\n",
    "\n",
    "# elif config['model_name'] == 'wgan-gp-data-aug':\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17f5ea6f44959d3e14a0023402df6111059cc6a22e6f41b4a22d1b177e49012d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
