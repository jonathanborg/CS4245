{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Package Setup and Initialization\n","Import all required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch as th\n","import torchvision\n","from torch.utils.data import DataLoader\n","\n","import os\n","import matplotlib.pyplot as plt\n","import torchvision.utils as vutils\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["# Setup configuration\n","Setup hyperparameters for the network to use"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Network\n","NOISE_SIZE = 100\n","NOISE_TYPE = 'normal' \n","CRITIC_FEATURE_MAP_DEPTH = 64               # in WGAN the Discriminator is called the Critic\n","GENERATOR_FEATURE_MAP_DEPTH = 64\n","\n","# Training \n","SAVE_CHECKPOINT_EVERY = 10 \n","SAVE_IMAGE_EVERY = 10\n","BATCH_SIZE = 64\n","EPOCHS = 1000\n","DISCRIMINATOR_LR = 1e-4\n","GENERATOR_LR = 1e-4 \n","TRUE_LABEL_VALUE = 1\n","FAKE_LABEL_VALUE = 0\n","\n","# WGAN params\n","NUM_EPOCHS = 5\n","CRITIC_ITERATIONS = 5\n","WEIGHT_CLIP = 0.1"]},{"cell_type":"markdown","metadata":{},"source":["# Setup device and data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Device\n","device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n","\n","# Dataset\n","transform = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","])\n","\n","data_directory = \"/kaggle/input/\"\n","dataset = torchvision.datasets.ImageFolder(data_directory, transform=transform)\n","\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Network\n","Critic (Discriminator) and Generator\n","Note that the Critic in WGAN doest not have a sigmoid activation function in its last layer as opposed to the DCGAN variant. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DISCRIMINATOR\n","class CriticBlock(th.nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, first: bool = False, last: bool = False) -> None:\n","        assert(not (first and last)) # block can't be both first and last\n","        super().__init__()\n","        if first:\n","            self.main = th.nn.Sequential(\n","                th.nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n","                th.nn.LeakyReLU(0.2, inplace=True),\n","            )\n","            \n","        elif last:\n","            self.main = th.nn.Sequential(\n","                th.nn.Conv2d(in_channels, out_channels, 3, 1, 0, bias=False),\n","                # No Sigmoid activation in WGAN in last layer\n","            )\n","\n","        else:\n","            self.main = th.nn.Sequential(\n","                th.nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n","                th.nn.BatchNorm2d(out_channels),\n","                th.nn.LeakyReLU(0.2, inplace=True),\n","            )\n","\n","    def forward(self, x: th.Tensor) -> th.Tensor:\n","        return self.main(x)\n","\n","class Critic(th.nn.Module):\n","    def __init__(self, feature_map_depth: int) -> None:\n","        super().__init__()\n","        self.main = th.nn.Sequential(\n","            CriticBlock(3, feature_map_depth, first=True),\n","            CriticBlock(feature_map_depth, feature_map_depth * 2),\n","            CriticBlock(feature_map_depth * 2, feature_map_depth * 4),\n","            CriticBlock(feature_map_depth * 4, feature_map_depth * 8),\n","            CriticBlock(feature_map_depth * 8, feature_map_depth * 8),\n","            CriticBlock(feature_map_depth * 8, 1, last=True)\n","        )\n","\n","    def forward(self, x: th.Tensor) -> th.Tensor:\n","        x = self.main(x)\n","        return x\n","\n","\n","\n","# GENERATOR\n","class GeneratorBlock(th.nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, first: bool = False, last: bool = False) -> None:\n","        assert(not (first and last)) # block can't be both first and last\n","        super().__init__()\n","        if first:\n","            self.main = th.nn.Sequential(\n","                th.nn.ConvTranspose2d(in_channels, out_channels, 3, 1, 0, bias=False),\n","                th.nn.BatchNorm2d(out_channels),\n","                th.nn.ReLU(True)\n","            )\n","        elif last:\n","            self.main = th.nn.Sequential(\n","                th.nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n","                th.nn.Tanh()\n","            )\n","        else:\n","            self.main = th.nn.Sequential(\n","                th.nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n","                th.nn.BatchNorm2d(out_channels),\n","                th.nn.ReLU(True)\n","            )\n","\n","    def forward(self, x: th.Tensor) -> th.Tensor:\n","        return self.main(x)\n","\n","class Generator(th.nn.Module):\n","    def __init__(self, noise_size: int, feature_map_depth: int) -> None:\n","        super().__init__()\n","        # first layer, no stride. Upsample from 1x1 to 4x4\n","        self.main = th.nn.Sequential(\n","            GeneratorBlock(noise_size, feature_map_depth * 8, first=True),\n","            GeneratorBlock(feature_map_depth * 8, feature_map_depth * 8),\n","            GeneratorBlock(feature_map_depth * 8, feature_map_depth * 4),\n","            GeneratorBlock(feature_map_depth * 4, feature_map_depth * 2),\n","            GeneratorBlock(feature_map_depth * 2, feature_map_depth * 1),\n","            GeneratorBlock(feature_map_depth * 1, 3, last=True),\n","        )\n","\n","    def forward(self, x: th.Tensor) -> th.Tensor:\n","        x = self.main(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["# Optimizer and creating network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize weights\n","def weights_init(model):\n","    classname = model.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        th.nn.init.normal_(model.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        th.nn.init.normal_(model.weight.data, 1.0, 0.02)\n","        th.nn.init.constant_(model.bias.data, 0)\n","\n","\n","# Create network\n","generator = Generator(NOISE_SIZE, GENERATOR_FEATURE_MAP_DEPTH).to(device)\n","generator.apply(weights_init)\n","\n","critic = Critic(CRITIC_FEATURE_MAP_DEPTH).to(device)\n","critic.apply(weights_init)\n","\n","# Optimizer\n","critic_optimizer = th.optim.RMSprop(critic.parameters(), lr=DISCRIMINATOR_LR)\n","generator_optimizer = th.optim.RMSprop(generator.parameters(), lr=GENERATOR_LR)"]},{"cell_type":"markdown","metadata":{},"source":["# Utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Constants\n","results_path = \"kaggle\"\n","experiment_name = \"working\"\n","FULL_PATH = f'{results_path}/{experiment_name}'\n","fixed_noise = th.randn(64, NOISE_SIZE, 1, 1, device=device)\n","\n","# Utility functions\n","def save_model_checkpoint(epoch: int) -> None:\n","    make_epoch_directories(epoch)\n","    checkpoint_path = f'{FULL_PATH}/{epoch}'\n","    th.save({\n","        'epoch': epoch,\n","        'generator_model_state_dict': generator.state_dict(),\n","        'discriminator_model_state_dict': critic.state_dict(),\n","        'generator_optimizer_state_dict': generator_optimizer.state_dict(),\n","        'discriminator_optimizer_state_dict': critic_optimizer.state_dict(),\n","    }, f'{checkpoint_path}/checkpoint.th')\n","\n","\n","def make_epoch_directories(epoch: int) -> None:\n","    epoch_path = f'{FULL_PATH}/{epoch}'\n","    if not os.path.isdir(epoch_path):\n","        os.mkdir(epoch_path)\n","\n","\n","def save_model_image(epoch: int) -> None:\n","    make_epoch_directories(epoch)\n","    image_path = f'{FULL_PATH}/{epoch}/images'\n","    if not os.path.isdir(image_path):\n","        os.mkdir(image_path)\n","    random_noise = th.randn(64, NOISE_SIZE, 1, 1, device=device)\n","    fixed_fakes = generator(fixed_noise).detach().cpu()\n","    random_fakes = generator(random_noise).detach().cpu()\n","    save_image_grid(fixed_fakes, f'{image_path}/fixed.png', 'Fixed Noise')\n","    save_image_grid(random_fakes, f'{image_path}/random.png', 'Random Noise')\n","\n","\n","def save_image_grid(images, path: str, title: str) -> None:\n","    plt.figure(figsize=(8,8))\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.imshow(np.transpose(vutils.make_grid(images.to(device)[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))\n","    plt.savefig(path)\n","    plt.close()"]},{"cell_type":"markdown","metadata":{},"source":["# Training loop\n","Main training loop. Note that we use the algorithm outlined in the WGAN paper. So in this case the Critic is updated more frequently (5x) and we use the Loss Function descrbied in the paper (and other resources online see: https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/) \n","\n","From this blogpost:\n","    Critic Loss = [average critic score on real images] â€“ [average critic score on fake images]\n","    Generator Loss = -[average critic score on fake images]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Critic_losses = []\n","Generator_losses = []\n","\n","for epoch in range(EPOCHS):\n","    print('EPOCH: ', epoch)\n","    for batch_idx, (real, _) in enumerate(dataloader):\n","        print('\\t batch:', batch_idx)\n","        real = real.to(device)\n","        batch_size = real.size(0)\n","        \n","        # TRAIN DISCRIMINATOR (CRITIC) MORE. (5x according to paper)\n","        for _ in range(CRITIC_ITERATIONS):\n","            noise = th.randn(batch_size, NOISE_SIZE, 1, 1, device=device)\n","            global fake \n","            fake = generator(noise)\n","            \n","            discriminator_fake = critic(fake).reshape(-1)\n","    \n","            discriminator_real = critic(real).reshape(-1)\n","            \n","            # extra '-' because originally we want to maximize, so we minimize the negative.\n","            loss_discriminator = -(th.mean(discriminator_fake) - th.mean(discriminator_real))\n","            \n","            critic.zero_grad()\n","            loss_discriminator.backward(retain_graph=True)\n","            critic_optimizer.step()\n","\n","            for p in critic.parameters():\n","                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n","\n","        # TRAIN GENERATOR \n","        output = critic(fake).reshape(-1)\n","        loss_generator = -th.mean(output)\n","        generator.zero_grad() \n","        loss_generator.backward()\n","        generator_optimizer.step()\n","\n","        # Output training stats\n","\n","    # SAVE IMAGES\n","    if epoch % SAVE_CHECKPOINT_EVERY == 0:\n","        print('-> Saving model checkpoint')\n","        save_model_checkpoint(epoch)\n","    \n","    # if epoch % SAVE_IMAGE_EVERY == 0:\n","        print('-> Saving model images')\n","        save_model_image(epoch)"]}],"metadata":{"interpreter":{"hash":"0710d308d66b8004f73af2ec28aa1130c98ac659d02c9b147a320fe9d85a0088"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
